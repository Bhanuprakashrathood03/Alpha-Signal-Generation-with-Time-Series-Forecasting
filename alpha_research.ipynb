{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Research\n",
    "\n",
    "The goal in this project is to define a liquid universe of stocks where we would apply our factors into it to see through our factor analysis if there is a potential or not to send these results to [production](https://github.com/keyvantaj/Quantitative/blob/master/portfolio_management.ipynb). After selecting and combining factors using Machine Learning technics, the combined factor is analyzed and improved with an optimizer function to integrate our risk model.  \n",
    "\n",
    "This project workflow is comprised of distinct stages including: \n",
    "\n",
    "1. Parameters\n",
    "2. Universe definition\n",
    "3. Sector definition\n",
    "4. Alpha factors\n",
    "5. Factor analysis\n",
    "6. Factors combination\n",
    "7. Risk analysis for equal weights\n",
    "8. Integrating factor data to the optimizer\n",
    "9. Optimized alpha vector analysis \n",
    "10. Predicted portfolio\n",
    "\n",
    "In this context we have used different source of data provided from \n",
    "[Sharadar](https://www.quandl.com/publishers/sharadar) and \n",
    "[IFT](https://www.quandl.com/publishers/ift) as described below:\n",
    "\n",
    "- Sharadar Equity Prices ([SHARADAR/SEP](https://www.quandl.com/databases/SEP/data))\n",
    "Updated daily,End-Of-Day (EOD) price (ohlcv) data for more than 14,000 US public companies.  \n",
    "- Indicator Descriptions ([SHARADAR/INDICATORS](https://www.quandl.com/databases/SF1/data))\n",
    "Description of indicators listed in SF1 table for more than 14,000 US public companies.\n",
    "- Tickers and Metadata ([SHARADAR/TICKERS](https://www.quandl.com/databases/SF1/data))\n",
    "Information and metadata for more than 14,000 US public companies.\n",
    "- Core US Fundamentals ([SHARADAR/SF1](https://www.quandl.com/databases/SF1/data))\n",
    " 150 essential fundamental indicators and financial ratios, for more than 14,000 US public companies.\n",
    "- Daily Metrics ([SHARADAR/DAILY](https://www.quandl.com/databases/SF1/data))\n",
    " 5 essential metrics indicators and financial ratios daily updated, for more than 14,000 US public companies.\n",
    "- Sentiment Analysis and News Analytics ([IFT/NSA](https://www.quandl.com/databases/NS1/data)) \n",
    "News, blogs, social media and proprietary sources for thousands of stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tickers and Metadata [SHARADAR/TICKERS] features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>table</b> : Sharadar Table : The database table which the ticker is featured in. Examples are: \"SF1\" or \"SEP. \n",
    "\n",
    "- <b>permaticker</b> : Permanent Ticker Symbol : The permaticker is a unique and unchanging identifier for an issuer in the dataset which is issued by Sharadar. \n",
    "\n",
    "- <b>name</b> : Issuer Name : The name of the security issuer. \n",
    "\n",
    "- <b>exchange</b> : Stock Exchange : The exchange on which the security trades. Examples are: \"NASDAQ\";\"NYSE\";\"NYSEARCA\";\"BATS\";\"OTC\" and \"NYSEMKT\" (previously the American Stock exchange). \n",
    "\n",
    "- <b>isdelisted</b> : Is Delisted? : Is the security delisted? [Y]es or [N]o. \n",
    "\n",
    "- <b>category</b> : Issuer Category : The category of the issuer: \"Domestic\"; \"Canadian\" or \"ADR\". \n",
    "\n",
    "- <b>cusips</b> : CUSIPs : A security identifier. Space delimited in the event of multiple identifiers. \n",
    "\n",
    "- <b>siccode</b> : Standard Industrial Classification (SIC) Code : The Standard Industrial Classification (SIC) is a system for classifying industries by a four-digit code; as sourced from SEC filings. More on the SIC system here: https://en.wikipedia.org/wiki/Standard_Industrial_Classification  \n",
    "\n",
    "- <b>sicsector</b> : SIC Sector : The SIC sector is based on the SIC code and the division tabled here: https://en.wikipedia.org/wiki/Standard_Industrial_Classification  \n",
    "\n",
    "- <b>sicindustry</b> : SIC Industry : The SIC industry is based on the SIC code and the industry tabled here: https://www.sec.gov/info/edgar/siccodes.htm \n",
    "\n",
    "- <b>famasector</b> : Fama Sector : Not currently active - coming in a future update. \n",
    "\n",
    "- <b>famaindustry</b> : Fama Industry : Industry classifications based on the SIC code and classifications by Fama and French here: http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/det_48_ind_port.html \n",
    "\n",
    "- <b>sector</b> : Sector : Sharadar's sector classification based on SIC codes in a format which approximates to GICS. \n",
    "\n",
    "- <b>industry</b> : Industry : Sharadar's industry classification based on SIC codes in a format which approximates to GICS. \n",
    "\n",
    "- <b>scalemarketcap</b> : Company Scale - Market Cap : This field is experimental and subject to change. It categorises the company according to it's maximum observed market cap as follows: 1 - Nano < 50m; 2 - Micro < 300m; 3 - Small < 2bn; 4 - Mid < 10bn; 5 - Large < 200bn; 6 - Mega >= 200bn \n",
    "\n",
    "- <b>scalerevenue</b> : Company Scale - Revenue : This field is experimental and subject to change. It categorises the company according to it's maximum observed annual revenue as follows: 1 - Nano < 50m; 2 - Micro < 300m; 3 - Small < 2bn; 4 - Mid < 10bn; 5 - Large < 200bn; 6 - Mega >= 200bn \n",
    "\n",
    "- <b>relatedtickers</b> : Related Tickers : Where related tickers have been identified this field is populated. Related tickers can include the prior ticker before a ticker change; and it tickers for alternative share classes. \n",
    "\n",
    "- <b>currency</b> : Currency : The company functional reporting currency for the SF1 Fundamentals table or the currency for EOD prices in SEP and SFP. \n",
    "\n",
    "- <b>location</b> : Location : The company location as registered with the Securities and Exchange Commission. \n",
    "\n",
    "- <b>lastupdated</b> : Last Updated Date : Last Updated represents the last date that this database entry was updated; which is useful to users when updating their local records. \n",
    "\n",
    "- <b>firstadded</b> : First Added Date : The date that the ticker was first added to coverage in the dataset. \n",
    "\n",
    "- <b>firstpricedate</b> : First Price Date : The date of the first price observation for a given ticker. Can be used as a proxy for IPO date. Minimum value of 1986-01-01 for IPO's that occurred prior to this date. Note: this does not necessarily represent the first price date available in our datasets since our end of day price history currently starts in December 1998. \n",
    "\n",
    "- <b>lastpricedate</b> : Last Price Date : The most recent price observation available. \n",
    "\n",
    "- <b>firstquarter</b> : First Quarter : The first financial quarter available in the dataset. \n",
    "\n",
    "- <b>lastquarter</b> : Last Quarter : The last financial quarter available in the dataset. \n",
    "\n",
    "- <b>secfilings</b> : SEC Filings URL : The URL pointing to the SEC filings which also contains the Central Index Key (CIK). \n",
    "\n",
    "- <b>companysite</b> : Company Website URL : The URL pointing to the company website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core US Fundamentals [SHARADAR/SF1] features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>accoci</b> : Accumulated Other Comprehensive Income : [Balance Sheet] A component of [Equity] representing the accumulated change in equity from transactions and other events and circumstances from non-owner sources; net of tax effect; at period end. Includes foreign currency translation items; certain pension adjustments; unrealized gains and losses on certain investments in debt and equity securities. \n",
    "\n",
    "- <b>assets</b> : Total Assets : [Balance Sheet] Sum of the carrying amounts as of the balance sheet date of all assets that are recognized. Major components are [CashnEq]; [Investments];[Intangibles]; [PPNENet];[TaxAssets] and [Receivables]. \n",
    "\n",
    "- <b>assetsc</b> : Current Assets : [Balance Sheet] The current portion of [Assets]; reported if a company operates a classified balance sheet that segments current and non-current assets. \n",
    "\n",
    "- <b>assetsnc</b> : Assets Non-Current : [Balance Sheet] Amount of non-current assets; for companies that operate a classified balance sheet. Calculated as the different between Total Assets [Assets] and Current Assets [AssetsC]. \n",
    "\n",
    "- <b>bvps</b> : Book Value per Share : [Metrics] Measures the ratio between [Equity] and [SharesWA] as adjusted by [ShareFactor]. \n",
    "\n",
    "- <b>capex</b> : Capital Expenditure : [Cash Flow Statement] A component of [NCFI] representing the net cash inflow (outflow) associated with the acquisition & disposal of long-lived; physical & intangible assets that are used in the normal conduct of business to produce goods and services and are not intended for resale. Includes cash inflows/outflows to pay for construction of self-constructed assets & software. \n",
    "\n",
    "- <b>cashneq</b> : Cash and Equivalents : [Balance Sheet] A component of [Assets] representing the amount of currency on hand as well as demand deposits with banks or financial institutions. \n",
    "\n",
    "- <b>cashnequsd</b> : Cash and Equivalents (USD) : [Balance Sheet] [CashnEq] in USD; converted by [FXUSD]. \n",
    "\n",
    "- <b>cor</b> : Cost of Revenue : [Income Statement] The aggregate cost of goods produced and sold and services rendered during the reporting period. \n",
    "\n",
    "- <b>consolinc</b> : Consolidated Income : [Income Statement] The portion of profit or loss for the period; net of income taxes; which is attributable to the consolidated entity; before the deduction of [NetIncNCI]. \n",
    "\n",
    "- <b>currentratio</b> : Current Ratio : [Metrics] The ratio between [AssetsC] and [LiabilitiesC]; for companies that operate a classified balance sheet. \n",
    "\n",
    "- <b>de</b> : Debt to Equity Ratio : [Metrics] Measures the ratio between [Liabilities] and [Equity]. \n",
    "\n",
    "- <b>debt</b> : Total Debt : [Balance Sheet] A component of [Liabilities] representing the total amount of current and non-current debt owed. Includes secured and unsecured bonds issued; commercial paper; notes payable; credit facilities; lines of credit; capital lease obligations; operating lease obligations; and convertible notes. \n",
    "\n",
    "- <b>debtc</b> : Debt Current : [Balance Sheet] The current portion of [Debt]; reported if the company operates a classified balance sheet that segments current and non-current liabilities. \n",
    "\n",
    "- <b>debtnc</b> : Debt Non-Current : [Balance Sheet] The non-current portion of [Debt] reported if the company operates a classified balance sheet that segments current and non-current liabilities. \n",
    "\n",
    "- <b>debtusd</b> : Total Debt (USD) : [Balance Sheet] [Debt] in USD; converted by [FXUSD]. \n",
    "\n",
    "- <b>deferredrev</b> : Deferred Revenue : [Balance Sheet] A component of [Liabilities] representing the carrying amount of consideration received or receivable on potential earnings that were not recognized as revenue; including sales; license fees; and royalties; but excluding interest income. \n",
    "\n",
    "- <b>depamor</b> : Depreciation Amortization & Accretion : [Cash Flow Statement] A component of operating cash flow representing the aggregate net amount of depreciation; amortization; and accretion recognized during an accounting period. As a non-cash item; the net amount is added back to net income when calculating cash provided by or used in operations using the indirect method. \n",
    "\n",
    "- <b>deposits</b> : Deposit Liabilities : [Balance Sheet] A component of [Liabilities] representing the total of all deposit liabilities held; including foreign and domestic; interest and noninterest bearing. May include demand deposits; saving deposits; Negotiable Order of Withdrawal and time deposits among others. \n",
    "\n",
    "- <b>divyield</b> : Dividend Yield : [Metrics] Dividend Yield measures the ratio between a company's [DPS] and its [Price]. \n",
    "\n",
    "- <b>dps</b> : Dividends per Basic Common Share : [Income Statement] Aggregate dividends declared during the period for each split-adjusted share of common stock outstanding. Includes spinoffs where identified. \n",
    "\n",
    "- <b>ebit</b> : Earning Before Interest & Taxes (EBIT) : [Income Statement] Earnings Before Interest and Tax is calculated by adding [TaxExp] and [IntExp] back to [NetInc]. \n",
    "\n",
    "- <b>ebitda</b> : Earnings Before Interest Taxes & Depreciation Amortization (EBITDA) : [Metrics] EBITDA is a non-GAAP accounting metric that is widely used when assessing the performance of companies; calculated by adding [DepAmor] back to [EBIT]. \n",
    "\n",
    "- <b>ebitdamargin</b> : EBITDA Margin : [Metrics] Measures the ratio between a company's [EBITDA] and [Revenue]. \n",
    "\n",
    "- <b>ebitdausd</b> : Earnings Before Interest Taxes & Depreciation Amortization (USD) : [Metrics] [EBITDA] in USD; converted by [FXUSD]. \n",
    "\n",
    "- <b>ebitusd</b> : Earning Before Interest & Taxes (USD) : [Income Statement] [EBIT] in USD; converted by [FXUSD]. \n",
    "\n",
    "- <b>ebt</b> : Earnings before Tax : [Metrics] Earnings Before Tax is calculated by adding [TaxExp] back to [NetInc]. \n",
    "\n",
    "- <b>eps</b> : Earnings per Basic Share : [Income Statement] Earnings per share as calculated and reported by the company. Approximates to the amount of [NetIncCmn] for the period per each [SharesWA] after adjusting for [ShareFactor]. \n",
    "\n",
    "- <b>epsdil</b> : Earnings per Diluted Share : [Income Statement] Earnings per diluted share as calculated and reported by the company. Approximates to the amount of [NetIncCmn] for the period per each [SharesWADil] after adjusting for [ShareFactor].. \n",
    "\n",
    "- <b>epsusd</b> : Earnings per Basic Share (USD) : [Income Statement] [EPS] in USD; converted by [FXUSD]. \n",
    "\n",
    "- <b>equity</b> : Shareholders Equity : [Balance Sheet] A principal component of the balance sheet; in addition to [Liabilities] and [Assets]; that represents the total of all stockholders' equity (deficit) items; net of receivables from officers; directors; owners; and affiliates of the entity which are attributable to the parent. \n",
    "\n",
    "- <b>equityusd</b> : Shareholders Equity (USD) : [Balance Sheet] [Equity] in USD; converted by [FXUSD]. \n",
    "\n",
    "- <b>ev</b> : Enterprise Value : [Metrics] Enterprise value is a measure of the value of a business as a whole; calculated as [MarketCap] plus [DebtUSD] minus [CashnEqUSD]. \n",
    "\n",
    "- <b>evebit</b> : Enterprise Value over EBIT : [Metrics] Measures the ratio between [EV] and [EBITUSD]. \n",
    "\n",
    "- <b>evebitda</b> : Enterprise Value over EBITDA : [Metrics] Measures the ratio between [EV] and [EBITDAUSD]. \n",
    "\n",
    "- <b>fcf</b> : Free Cash Flow : [Metrics] Free Cash Flow is a measure of financial performance calculated as [NCFO] minus [CapEx]. \n",
    "\n",
    "- <b>fcfps</b> : Free Cash Flow per Share : [Metrics] Free Cash Flow per Share is a valuation metric calculated by dividing [FCF] by [SharesWA] and [ShareFactor]. \n",
    "\n",
    "- <b>fxusd</b> : Foreign Currency to USD Exchange Rate : [Metrics] The exchange rate used for the conversion of foreign currency to USD for non-US companies that do not report in USD. \n",
    "\n",
    "- <b>gp</b> : Gross Profit : [Income Statement] Aggregate revenue [Revenue] less cost of revenue [CoR] directly attributable to the revenue generation activity. \n",
    "\n",
    "- <b>grossmargin</b> : Gross Margin : [Metrics] Gross Margin measures the ratio between a company's [GP] and [Revenue]. \n",
    "\n",
    "- <b>intangibles</b> : Goodwill and Intangible Assets : [Balance Sheet] A component of [Assets] representing the carrying amounts of all intangible assets and goodwill as of the balance sheet date; net of accumulated amortization and impairment charges. \n",
    "\n",
    "- <b>intexp</b> : Interest Expense : [Income Statement] Amount of the cost of borrowed funds accounted for as interest expense. \n",
    "\n",
    "- <b>invcap</b> : Invested Capital : [Metrics] Invested capital is an input into the calculation of [ROIC]; and is calculated as: [Debt] plus [Assets] minus [Intangibles] minus [CashnEq] minus [LiabilitiesC]. Please note this calculation method is subject to change. \n",
    "\n",
    "- <b>inventory</b> : Inventory : [Balance Sheet] A component of [Assets] representing the amount after valuation and reserves of inventory expected to be sold; or consumed within one year or operating cycle; if longer. \n",
    "\n",
    "- <b>investments</b> : Investments : [Balance Sheet] A component of [Assets] representing the total amount of marketable and non-marketable securties; loans receivable and other invested assets. \n",
    "\n",
    "- <b>investmentsc</b> : Investments Current : [Balance Sheet] The current portion of [Investments]; reported if the company operates a classified balance sheet that segments current and non-current assets. \n",
    "\n",
    "- <b>investmentsnc</b> : Investments Non-Current : [Balance Sheet] The non-current portion of [Investments]; reported if the company operates a classified balance sheet that segments current and non-current assets. \n",
    "\n",
    "- <b>liabilities</b> : Total Liabilities : [Balance Sheet] Sum of the carrying amounts as of the balance sheet date of all liabilities that are recognized. Principal components are [Debt]; [DeferredRev]; [Payables];[Deposits]; and [TaxLiabilities]. \n",
    "\n",
    "- <b>liabilitiesc</b> : Current Liabilities : [Balance Sheet] The current portion of [Liabilities]; reported if the company operates a classified balance sheet that segments current and non-current liabilities. \n",
    "\n",
    "- <b>liabilitiesnc</b> : Liabilities Non-Current : [Balance Sheet] The non-current portion of [Liabilities]; reported if the company operates a classified balance sheet that segments current and non-current liabilities. \n",
    "\n",
    "- <b>marketcap</b> : Market Capitalization : [Metrics] Represents the product of [SharesBas]; [Price] and [ShareFactor]. \n",
    "\n",
    "- <b>ncf</b> : Net Cash Flow / Change in Cash & Cash Equivalents : [Cash Flow Statement] Principal component of the cash flow statement representing the amount of increase (decrease) in cash and cash equivalents. Includes [NCFO]; investing [NCFI] and financing [NCFF] for continuing and discontinued operations; and the effect of exchange rate changes on cash [NCFX]. \n",
    "\n",
    "- <b>ncfbus</b> : Net Cash Flow - Business Acquisitions and Disposals : [Cash Flow Statement] A component of [NCFI] representing the net cash inflow (outflow) associated with the acquisition & disposal of businesses; joint-ventures; affiliates; and other named investments. \n",
    "\n",
    "- <b>ncfcommon</b> : Issuance (Purchase) of Equity Shares : [Cash Flow Statement] A component of [NCFF] representing the net cash inflow (outflow) from common equity changes. Includes additional capital contributions from share issuances and exercise of stock options; and outflow from share repurchases.  \n",
    "\n",
    "- <b>ncfdebt</b> : Issuance (Repayment) of Debt Securities  : [Cash Flow Statement] A component of [NCFF] representing the net cash inflow (outflow) from issuance (repayment) of debt securities. \n",
    "\n",
    "- <b>ncfdiv</b> : Payment of Dividends & Other Cash Distributions    : [Cash Flow Statement] A component of [NCFF] representing dividends and dividend equivalents paid on common stock and restricted stock units. \n",
    "\n",
    "- <b>ncff</b> : Net Cash Flow from Financing : [Cash Flow Statement] A component of [NCF] representing the amount of cash inflow (outflow) from financing activities; from continuing and discontinued operations. Principal components of financing cash flow are: issuance (purchase) of equity shares; issuance (repayment) of debt securities; and payment of dividends & other cash distributions. \n",
    "\n",
    "- <b>ncfi</b> : Net Cash Flow from Investing : [Cash Flow Statement] A component of [NCF] representing the amount of cash inflow (outflow) from investing activities; from continuing and discontinued operations. Principal components of investing cash flow are: capital (expenditure) disposal of equipment [CapEx]; business (acquisitions) disposition [NCFBus] and investment (acquisition) disposal [NCFInv]. \n",
    "\n",
    "- <b>ncfinv</b> : Net Cash Flow - Investment Acquisitions and Disposals : [Cash Flow Statement] A component of [NCFI] representing the net cash inflow (outflow) associated with the acquisition & disposal of investments; including marketable securities and loan originations. \n",
    "\n",
    "- <b>ncfo</b> : Net Cash Flow from Operations : [Cash Flow Statement] A component of [NCF] representing the amount of cash inflow (outflow) from operating activities; from continuing and discontinued operations. \n",
    "\n",
    "- <b>ncfx</b> : Effect of Exchange Rate Changes on Cash  : [Cash Flow Statement] A component of Net Cash Flow [NCF] representing the amount of increase (decrease) from the effect of exchange rate changes on cash and cash equivalent balances held in foreign currencies. \n",
    "\n",
    "- <b>netinc</b> : Net Income : [Income Statement] The portion of profit or loss for the period; net of income taxes; which is attributable to the parent after the deduction of [NetIncNCI] from [ConsolInc]; and before the deduction of [PrefDivIS]. \n",
    "\n",
    "- <b>netinccmn</b> : Net Income Common Stock : [Income Statement] The amount of net income (loss) for the period due to common shareholders. Typically differs from [NetInc] to the parent entity due to the deduction of [PrefDivIS]. \n",
    "\n",
    "- <b>netinccmnusd</b> : Net Income Common Stock (USD) : [Income Statement] [NetIncCmn] in USD; converted by [FXUSD]. \n",
    "\n",
    "- <b>netincdis</b> : Net Loss Income from Discontinued Operations : [Income Statement] Amount of loss (income) from a disposal group; net of income tax; reported as a separate component of income. \n",
    "\n",
    "- <b>netincnci</b> : Net Income to Non-Controlling Interests : [Income Statement] The portion of income which is attributable to non-controlling interest shareholders; subtracted from [ConsolInc] in order to obtain [NetInc]. \n",
    "\n",
    "- <b>netmargin</b> : Profit Margin : [Metrics] Measures the ratio between a company's [NetIncCmn] and [Revenue]. \n",
    "\n",
    "- <b>opex</b> : Operating Expenses : [Income Statement] Operating expenses represents the total expenditure on [SGnA]; [RnD] and other operating expense items; it excludes [CoR]. \n",
    "\n",
    "- <b>opinc</b> : Operating Income : [Income Statement] Operating income is a measure of financial performance before the deduction of [IntExp]; [TaxExp] and other Non-Operating items. It is calculated as [GP] minus [OpEx]. \n",
    "\n",
    "- <b>payables</b> : Trade and Non-Trade Payables : [Balance Sheet] A component of [Liabilities] representing trade and non-trade payables. \n",
    "\n",
    "- <b>payoutratio</b> : Payout Ratio : [Metrics] The percentage of earnings paid as dividends to common stockholders. - Calculated by dividing [DPS] by [EPSUSD]. \n",
    "\n",
    "- <b>pb</b> : Price to Book Value : [Metrics] Measures the ratio between [MarketCap] and [EquityUSD]. \n",
    "\n",
    "- <b>pe</b> : Price Earnings (Damodaran Method) : [Metrics] Measures the ratio between [MarketCap] and [NetIncCmnUSD] \n",
    "\n",
    "- <b>pe1</b> : Price to Earnings Ratio : [Metrics] An alternative to [PE] representing the ratio between [Price] and [EPSUSD]. \n",
    "\n",
    "- <b>ppnenet</b> : Property Plant & Equipment Net : [Balance Sheet] A component of [Assets] representing the amount after accumulated depreciation; depletion and amortization of physical assets used in the normal conduct of business to produce goods and services and not intended for resale. Includes Operating Right of Use Assets. \n",
    "\n",
    "- <b>prefdivis</b> : Preferred Dividends Income Statement Impact : [Income Statement] Income statement item reflecting dividend payments to preferred stockholders. Subtracted from Net Income to Parent [NetInc] to obtain Net Income to Common Stockholders [NetIncCmn]. \n",
    "\n",
    "- <b>price</b> : Share Price (Adjusted Close) : [Entity] The price per common share adjusted for stock splits but not adjusted for dividends; used in the computation of [PE1]; [PS1]; [DivYield] and [SPS]. \n",
    "\n",
    "- <b>ps</b> : Price Sales (Damodaran Method) : [Metrics] Measures the ratio between [MarketCap] and [RevenueUSD]. \n",
    "\n",
    "- <b>ps1</b> : Price to Sales Ratio : [Metrics] An alternative calculation method to [PS]; that measures the ratio between a company's [Price] and it's [SPS]. \n",
    "\n",
    "- <b>receivables</b> : Trade and Non-Trade Receivables : [Balance Sheet] A component of [Assets] representing trade and non-trade receivables. \n",
    "\n",
    "- <b>retearn</b> : Accumulated Retained Earnings (Deficit) : [Balance Sheet] A component of [Equity] representing the cumulative amount of the entities undistributed earnings or deficit. May only be reported annually by certain companies; rather than quarterly. \n",
    "\n",
    "- <b>revenue</b> : Revenues : [Income Statement] Amount of Revenue recognized from goods sold; services rendered; insurance premiums; or other activities that constitute an earning process. Interest income for financial institutions is reported net of interest expense and provision for credit losses. \n",
    "\n",
    "- <b>revenueusd</b> : Revenues (USD) : [Income Statement] [Revenue] in USD; converted by [FXUSD]. \n",
    "\n",
    "- <b>rnd</b> : Research and Development Expense : [Income Statement] A component of [OpEx] representing the aggregate costs incurred in a planned search or critical investigation aimed at discovery of new knowledge with the hope that such knowledge will be useful in developing a new product or service. \n",
    "\n",
    "- <b>sbcomp</b> : Share Based Compensation : [Cash Flow Statement] A component of [NCFO] representing the total amount of noncash; equity-based employee remuneration. This may include the value of stock or unit options; amortization of restricted stock or units; and adjustment for officers' compensation. As noncash; this element is an add back when calculating net cash generated by operating activities using the indirect method. \n",
    "\n",
    "- <b>sgna</b> : Selling General and Administrative Expense : [Income Statement] A component of [OpEx] representing the aggregate total costs related to selling a firm's product and services; as well as all other general and administrative expenses. Direct selling expenses (for example; credit; warranty; and advertising) are expenses that can be directly linked to the sale of specific products. Indirect selling expenses are expenses that cannot be directly linked to the sale of specific products; for example telephone expenses; Internet; and postal charges. General and administrative expenses include salaries of non-sales personnel; rent; utilities; communication; etc. \n",
    "\n",
    "- <b>sharefactor</b> : Share Factor : [Entity] Share factor is a multiplicant in the calculation of [MarketCap] and is used to adjust for: American Depository Receipts (ADRs) that represent more or less than 1 underlying share; and; companies which have different earnings share for different share classes (eg Berkshire Hathaway - BRK.B). \n",
    "\n",
    "- <b>sharesbas</b> : Shares (Basic) : [Entity] The number of shares or other units outstanding of the entity's capital or common stock or other ownership interests; as stated on the cover of related periodic report (10-K/10-Q); after adjustment for stock splits. \n",
    "\n",
    "- <b>shareswa</b> : Weighted Average Shares : [Income Statement] The weighted average number of shares or units issued and outstanding that are used by the company to calculate [EPS]; determined based on the timing of issuance of shares or units in the period. \n",
    "\n",
    "- <b>shareswadil</b> : Weighted Average Shares Diluted : [Income Statement] The weighted average number of shares or units issued and outstanding that are used by the company to calculate [EPSDil]; determined based on the timing of issuance of shares or units in the period. \n",
    "\n",
    "- <b>sps</b> : Sales per Share : [Metrics] Sales per Share measures the ratio between [RevenueUSD] and [SharesWA] as adjusted by [ShareFactor]. \n",
    "\n",
    "- <b>tangibles</b> : Tangible Asset Value : [Metrics] The value of tangibles assets calculated as the difference between [Assets] and [Intangibles]. \n",
    "\n",
    "- <b>taxassets</b> : Tax Assets : [Balance Sheet] A component of [Assets] representing tax assets and receivables. \n",
    "\n",
    "- <b>taxexp</b> : Income Tax Expense : [Income Statement] Amount of current income tax expense (benefit) and deferred income tax expense (benefit) pertaining to continuing operations. \n",
    "\n",
    "- <b>taxliabilities</b> : Tax Liabilities : [Balance Sheet] A component of [Liabilities] representing outstanding tax liabilities. \n",
    "\n",
    "- <b>tbvps</b> : Tangible Assets Book Value per Share : [Metrics] Measures the ratio between [Tangibles] and [SharesWA] as adjusted by [ShareFactor]. \n",
    "\n",
    "- <b>workingcapital</b> : Working Capital : [Metrics] Working capital measures the difference between [AssetsC] and [LiabilitiesC]. \n",
    "\n",
    "- <b>roe</b>: Return on Average Equity : [Metrics] Return on equity measures a corporation's profitability by calculating the amount of [NetIncCmn] returned as a percentage of [EquityAvg]. \n",
    "\n",
    "- <b>roa</b> : Return on Average Assets : [Metrics] Return on assets measures how profitable a company is [NetIncCmn] relative to its total assets [AssetsAvg]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharadar Equity Prices [SHARADAR/SEP] features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>open</b> : Open Price - Split Adjusted : The opening share price, adjusted for stock splits and stock dividends. \n",
    "- <b>high</b> : High Price - Split Adjusted : The high share price, adjusted for stock splits and stock dividends. \n",
    "- <b>low</b> : Low Price - Split Adjusted : The low share price, adjusted for stock splits and stock dividends. \n",
    "- <b>close</b> : Close Price - Split Adjusted : The open share closing, adjusted for stock splits and stock dividends. \n",
    "- <b>volume</b> : Volume - Split Adjusted : The traded volume, adjusted for stock splits and stock dividends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Metrics ([SHARADAR/DAILY] features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>ev</b> : Enterprise Value - Daily : Enterprise value is a measure of the value of a business as a whole; calculated as [MarketCap] plus [DebtUSD] minus [CashnEqUSD]. [MarketCap] is calculated by us, and the remaining figures are sourced from the most recent SEC form 10 filings. \n",
    "- <b>evebit</b> : Enterprise Value over EBIT - Daily : Measures the ratio between [EV] and [EBITUSD]. EBITUSD is derived from the most recent SEC form 10 filings. \n",
    "- <b>evebitda</b> : Enterprise Value over EBITDA - Daily : Measures the ratio between [EV] and [EBITDAUSD]. EBITDAUSD is derived from the most recent SEC form 10 filings. \n",
    "- <b>marketcap</b> : Market Capitalization - Daily : Represents the product of [SharesBas]; [Price] and [ShareFactor]. [SharesBas] is sourced from the most recent SEC form 10 filing. \n",
    "- <b>pb</b> : Price to Book Value - Daily : Measures the ratio between [MarketCap] and [EquityUSD]. [EquityUSD] is sourced from the most recent SEC form 10 filing. \n",
    "- <b>pe</b> : Price Earnings (Damodaran Method) - Daily : Measures the ratio between [MarketCap] and [NetIncCmnUSD]. [NetIncCmnUSD] is sourced from the most recent SEC form 10 filings. \n",
    "- <b>ps</b> : Price Sales (Damodaran Method) - Daily : Measures the ratio between [MarketCap] and [RevenueUSD]. [RevenueUSD] is sourced from the most recent SEC form 10 filings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis and News Analytics ([IFT/NSA] features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>sentiment Score</b>: a numeric measure of the bullishness / bearishness of news coverage of the stock.\n",
    "- <b>sentiment_high</b>: highest intraday sentiment scores.\n",
    "- <b>sentiment_low</b>: lowest intraday sentiment scores.\n",
    "- <b>news_volume</b>: the absolute number of news articles covering the stock.\n",
    "- <b>news_buzz</b>: a numeric measure of the change in coverage volume for the stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outsource packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pyfolio as pf\n",
    "import alphalens as al\n",
    "import zipfile\n",
    "import os\n",
    "import alphalens as al\n",
    "import quandl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from time import time,sleep\n",
    "from sklearn import preprocessing\n",
    "import pytz\n",
    "import itertools\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import yfinance as yf\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local packages\n",
    "from modules import FactorManagement as FM\n",
    "from modules import RiskManagement as RM\n",
    "from modules import Learner as LE\n",
    "from modules import OptimalHoldingsRegularization as OHR\n",
    "from modules import Util as UT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_key = pd.read_csv('secret_key.txt',header=None)\n",
    "quandl.ApiConfig.api_key = secret_key[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into factor research and quantitative analysis, we have to define parameters that will be used in different stages of this project. The purpose is to try different parameters to optimize our output portfolio during the time. The first parameter we set here below is `update_data` that is used to decide if we want to update the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to update data\n"
     ]
    }
   ],
   "source": [
    "update_data = True\n",
    "if update_data:\n",
    "    print ('ready to update data')\n",
    "else:\n",
    "    print ('data is already updated') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 1 - Time series data parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we set date parameters which are used for calling data from Quandl API. In this context, we call SEP/SHARADAR, DAILY/SHARADAR, and IFT/NSA for three years of data and SF1/SHARADAR for four years. The reason for this slicing is related to our final factor data where we look back one year. For example, some factors window length is set to one year, which means we need to load a minimum of three years data to chunk the final data frame in one year slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current date: 2020-11-14\n"
     ]
    }
   ],
   "source": [
    "tod = datetime.datetime.today().date()\n",
    "print ('current date: {}'.format(tod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SF1 data starting date: 2016-11-14\n"
     ]
    }
   ],
   "source": [
    "# SF1\n",
    "some_years = str(tod.year - 4)\n",
    "month = str(tod.month)\n",
    "day = str(tod.day)\n",
    "start_f = '{}-{}-{}'.format(some_years,month,day)\n",
    "print ('SF1 data starting date: {}'.format(start_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP,Daily and Sentiment data starting date: 2017-11-14\n"
     ]
    }
   ],
   "source": [
    "# SEP & daily\n",
    "two_years = str(tod.year - 3)\n",
    "month = str(tod.month)\n",
    "day = str(tod.day)\n",
    "start_sep = '{}-{}-{}'.format(two_years,month,day)\n",
    "print ('SEP,Daily and Sentiment data starting date: {}'.format(start_sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 2 - Factor data period parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set our final date parameter to one year factor data. The reason is that we believe more than one-year factor analysis will decrease our efficiency in prediction and could affect our interpretation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final slicing date for 1 year: 2019-11-14\n"
     ]
    }
   ],
   "source": [
    "# Slicing data for 1y\n",
    "years_to_slice = 1\n",
    "year = str(tod.year - years_to_slice)\n",
    "month = str(tod.month)\n",
    "day = str(tod.day)\n",
    "ayear = '{}-{}-{}'.format(year,month,day)\n",
    "\n",
    "start = ayear\n",
    "end = str(tod)\n",
    "print ('Final slicing date for {} year: {}'.format(years_to_slice,ayear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 3 - Universe parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The universe definition is an important step in this project. These parameters set here below will impact our analysis and need to be tuned as well as other parameters. metadata parameters are composed of `cap_select`, `exchange_select`, `currency_select` and `delisted_select` which are used to define our fisrt universe of stocks. For the second universe, `filteration_number` represents the number of liquid securities selected by dollar volume function and `smoothing_universe_period` represents the moving average window length in dollar volume function to select liquid securities smoothed over time. The following parameters are selected from the following values:\n",
    "\n",
    "#### Market Cap:\n",
    "\n",
    "- 1 - Nano\n",
    "- 2 - Micro \n",
    "- 3 - Small \n",
    "- 4 - Mid \n",
    "- 5 - Large \n",
    "- 6 - Mega\n",
    "\n",
    "#### Exhcange:\n",
    "\n",
    "- NASDAQ \n",
    "- NYSE\n",
    "- BATS\n",
    "- NYSEARCA\n",
    "- NYSEMKT\n",
    "- OTC\n",
    "\n",
    "#### Currency:\n",
    "\n",
    "USD, EUR, ARS, AUD, BRL, CAD, CHF, CLP, <br>\n",
    "CNY, COP, DKK, GBP, HKD, IDR, ILS, INR, <br>\n",
    "JPY, KRW, MXN, MYR, NOK, NZD, PEN,PHP, <br>\n",
    "PLN, RUB, SEK, TRY, TWD, ZAR\n",
    "\n",
    "#### Delisted:\n",
    "\n",
    "'Y' or 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata parameters\n",
    "cap_select = ['6 - Mega', '5 - Large', '4 - Mid']\n",
    "exchange_select = ['NYSE','NASDAQ','BATS']\n",
    "currency_select = ['USD']\n",
    "delisted_select = ['N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dollar volume parameters\n",
    "filteration_number  = 800\n",
    "smoothing_universe_period = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 4 - Pipeline parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating factor data in a different way to obtain significant results is a critical subject treated in this project. The goal is to tune up the factor parameter to get optimal results. This section lets us test different parameters for these factors and navigates over different factor data. `smoothed_value` is the window length moving average parameter used to remove the noise created by factor variation. We will discuss other parameters in the factors section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_value = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_in = ['ncf']\n",
    "momentum_in = {'momentum_252d':252}\n",
    "sma_in = {'sma200':200}\n",
    "daily_in = {'marketcap':120, 'evebitda':100, 'ps':100, 'pe':100, 'pb':100}\n",
    "over_in = {'overnight_sentiment_60d':60}\n",
    "direction_in = {'direction_100d':100}\n",
    "sent_in = {'sentiment_10d':10,'sentiment_60d':60}\n",
    "vol_in = {'volatility_5d':5,'volatility_20d':20}\n",
    "capm_in = {'capm_60d':60,'capm_20d':20,'capm_10d':10,'capm_5d':5}\n",
    "channels_in = {'chan_60d':60, 'chan_100d':100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 5 - Factor analysis parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below we select periods to analyze our multi-factor output. These periods are selected according to our trading strategy and portfolio management methods. The `rebalance_period` is also an important parameter to choose carefully and consider commission fees in our portfolio management system. \n",
    "\n",
    "We use [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) in our risk management model to reduce the dimensionality of the risk factors. `factor exposures` parameter is the dimension that we want to reduce to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_periods = (5,10,20)\n",
    "rebalance_period = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk analysis\n",
    "factor_exposures = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 6 - Optimizer parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After combining factors we put it into the optimizer function which will maximize alpha factor and consider our risk model in counterpart. `risk_cap` is used to set the risk exposure parameter. More the risk is and more the alpha will be. `lambda_reg` is used regularized our optimized function. it is operating like a portfolio enhancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_cap = 0.07\n",
    "lambda_reg = 0.5\n",
    "factor_max = 10\n",
    "factor_min = -10 \n",
    "weights_max = 0.2\n",
    "weights_min = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lambda_reg < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 7 - Sector parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our factor data is preprocessed and grouped by sector. Here below we can select sectors to drop, `sec_to_drop` regrouped sectors to not appear in factor data and our analysis to avoid sectors which have poor results. We also have the choice to drop specific sectors from long universe or short universe. For example the Tehnology sector shows good results in long qunatile and poor results in short quantile, it would be interesting to add this sector to `drop_short_sec`. The following parameter is selected from the following values:\n",
    "\n",
    "#### Sectors:\n",
    "\n",
    "- Basic Materials\n",
    "- Communication Services\n",
    "- Consumer Cyclical\n",
    "- Consumer Defensive\n",
    "- Energy\n",
    "- Financial Services\n",
    "- Healthcare\n",
    "- Industrials\n",
    "- Real Estate\n",
    "- Technology\n",
    "- Utilities\n",
    "- None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_to_drop = ['Communication Services']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_long_sec = []\n",
    "drop_short_sec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(sec_to_drop) & set(drop_long_sec) == set()\n",
    "assert set(sec_to_drop) & set(drop_short_sec) == set()\n",
    "assert set(drop_long_sec) & set(drop_short_sec) == set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 9 - Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here after the quantiles parameters are defining the equal portions of datas to be analyzed and considered for the future portfolio. `quantile_portions` is the number of qunatiles we want to analyze and work with. In the other hand `quantile_to_analysis` is the quantiles selected for the final analyze and portfolio. We use to select extremety quantiles to get the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qunatile_portions = 10\n",
    "quantile_to_analyse = [1,qunatile_portions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Universe definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the universe is defined as a group of assets having high liquidy over a period. This universe will be used to compare systematic factors. Before proceeding to the selection of stocks we use `get_table` from Quandl API to load metadata of all stocks.\n",
    "\n",
    "In the next step, we use our universe parameters to select the fisrt universe of stocks. The criteria for this selection are as followed:\n",
    "\n",
    "- Exchange\n",
    "- Market cap\n",
    "- Currency\n",
    "- Delisted\n",
    "\n",
    "The following codes are used to define the first universe described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 1 - Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = quandl.get_table('SHARADAR/TICKERS', table='SF1',paginate=True)\n",
    "meta.set_index('ticker',inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 1 - 1 - First universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exchange_select:\n",
    "    exchange = []\n",
    "    for i in exchange_select:\n",
    "        exchange.append(list(meta[(meta['exchange'] == i)].index))\n",
    "    meta_ex = meta.loc[list(itertools.chain.from_iterable(exchange))]\n",
    "else:\n",
    "    meta_ex = meta\n",
    "\n",
    "if currency_select:\n",
    "    currency = []\n",
    "    for i in currency_select:\n",
    "        currency.append(list(meta_ex[(meta_ex['currency'] == i)].index))\n",
    "    meta_ex_cu = meta_ex.loc[list(itertools.chain.from_iterable(currency))]\n",
    "else:\n",
    "    meta_ex_cu = meta_ex\n",
    "\n",
    "if delisted_select:\n",
    "    delisted = []\n",
    "    for i in delisted_select:\n",
    "        delisted.append(list(meta_ex_cu[(meta_ex_cu['isdelisted'] == i)].index))\n",
    "    meta_ex_cu_de = meta_ex_cu.loc[list(itertools.chain.from_iterable(delisted))]\n",
    "else:\n",
    "    meta_ex_cu_de = meta_ex_cu\n",
    "    \n",
    "if cap_select:    \n",
    "    cap = []\n",
    "    for i in cap_select:\n",
    "        cap.append(list(meta_ex_cu_de[(meta_ex_cu_de['scalemarketcap'] == i)].index))\n",
    "    meta_ex_cu_de_cap = meta_ex_cu_de.loc[list(itertools.chain.from_iterable(cap))]\n",
    "else:\n",
    "    meta_ex_cu_de_cap = meta_ex_cu_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = list(meta_ex_cu_de_cap.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040 assets selected in first selection\n"
     ]
    }
   ],
   "source": [
    "print ('{} assets selected in first selection'.format(len(u1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 2 - OHLCV data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `first_universe` is defined, we use it to get SHARADAR/SEP data and store this table as a zip file on the local drive. This table gives us the ohlcv data started at `start_sep` defined in the parameters section and ended at the current date `tod`. The zip file is extracted, sorted into a multi-index data frame, and finally cleaned using our local function `cleaning_dataframe` imported from `utils_s.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_data:\n",
    "    quandl.export_table('SHARADAR/SEP',\n",
    "                        ticker = u1, \n",
    "                        date = {'gte': start_sep, 'lte': str(end)}, \n",
    "                        filename = 'data/ohlcv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/ohlcv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "for item in os.listdir(os.getcwd()):  # loop through items in dir\n",
    "    if item.endswith('.csv') and item.split('_')[0] == 'SHARADAR' and item.split('_')[1] == 'SEP':\n",
    "        \n",
    "        ohlcv = pd.read_csv(item)\n",
    "        ohlcv['date'] = pd.to_datetime(ohlcv['date'])\n",
    "        ohlcv = ohlcv.set_index(['date', 'ticker']).sort_index(level=[0,1], ascending=[True, False])\n",
    "        ohlcv.drop(['lastupdated','dividends','closeunadj'],axis=1,inplace=True)\n",
    "\n",
    "        os.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv = UT.cleaning_dataframe(df = ohlcv,\n",
    "                              pernan_to_drop = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 2 - 1 - Second universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the `dollar_volume_universe` attends to select liquid stocks with significant market cap. In this context, the close frame is multiplied to volume to obtain the market cap data frame. Then we sort assets having the highest market cap. the number of assets selected is set in the parameters section through `filteration_number` and smoothed over time with `smoothing_universe_period`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dollar_volume_universe(tickers_num, ohlcv, sma_period):\n",
    "    \n",
    "    ohlcv['dollar_volume'] = ohlcv['close']*ohlcv['volume']\n",
    "    dollar_vol = ohlcv['dollar_volume'].unstack('ticker')\n",
    "    \n",
    "    sma = FM().sma(dollar_vol,sma_period)\n",
    "    \n",
    "    last = sma.iloc[-1,:]\n",
    "    dol = pd.DataFrame(data = last.values,index = last.index, columns = ['dv'])\n",
    "    dol.dropna(inplace = True)\n",
    "    \n",
    "    return list(dol.sort_values(by='dv', ascending=False).iloc[:tickers_num].index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = dollar_volume_universe(tickers_num = filteration_number, ohlcv = ohlcv, sma_period = smoothing_universe_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} assets selected out of {} for the second selection'.format(len(universe),len(u1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(universe) == filteration_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 2 - 2 - Third universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sector selection model is a systematic tool that tilts a portfolio towards sectors that are predicted to outperform and underweights those that are predicted to underperform. If the process can effectively discern winning/losing industry groups, it can enhance the value added from a stock-selection methodology. Here below we use `sec_to_drop` which is composed of sectors to not include in the final assets (third universe). the output of this function will be the final selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_sectors = pd.DataFrame(index=universe, columns=['sectors'])\n",
    "for i in universe:\n",
    "    try:\n",
    "        universe_sectors.loc[i] = meta_ex_cu_de_cap.loc[i]['sector']\n",
    "    except:\n",
    "        universe_sectors.loc[i] = np.nan\n",
    "    try:\n",
    "        for sec in sec_to_drop:\n",
    "            if meta_ex_cu_de_cap.loc[i]['sector'] == sec:\n",
    "                universe_sectors.drop(i, axis=0,inplace=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = list(universe_sectors.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print ('{} assets selected after sector cleaning out of {} for the third selection'.format(len(universe),len(u1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 3 - Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to define the universe, we have to prepare the benchmark. For this project the <b>S&P500</b> will be the reference index and we will use this data for our factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# input\n",
    "market = '^GSPC'\n",
    "# read data \n",
    "dfm = yf.download(market,ohlcv.index.levels[0][0],ohlcv.index.levels[0][-1] + pd.Timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "dfm = dfm.rename(columns={'Open': 'open', \n",
    "                          'High': 'high', \n",
    "                          'Low':'low', \n",
    "                          'Close':'close',\n",
    "                          'Volume':'volume'}).drop('Adj Close', axis=1)\n",
    "dfm.index.name = 'date'\n",
    "# return\n",
    "benchmark = dfm['close'].pct_change().loc[slice(start,end)]\n",
    "benchmark.index = benchmark.index.tz_localize('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 4 - Fundamental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the final `universe`, we get fundamental SHARADAR/SF1 data and store this table as a zip file on the local drive. This table gives us the fundamental data of Most Recent Quarter (MRQ) Reported started at `start_f` defined in the parameters section and ended at the current date `tod`. The zip file is extracted, sorted into a multi-index data frame, and finally cleaned using our local function `cleaning_dataframe` imported from `utils_s.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_data:\n",
    "    #qopts={\"columns\":ind}\n",
    "    quandl.export_table('SHARADAR/SF1',\n",
    "                        ticker=universe, \n",
    "                        dimension = 'MRQ', \n",
    "                        calendardate={'gte':start_f,'lte':str(tod)}, \n",
    "                        filename='data/fundamental.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/fundamental.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "for item in os.listdir(os.getcwd()):  # loop through items in dir\n",
    "    \n",
    "    if item.endswith('.csv') and item.split('_')[0] == 'SHARADAR' and item.split('_')[1] == 'SF1':\n",
    "        sf1 = pd.read_csv(item)\n",
    "        sf1 = sf1.set_index(['calendardate', 'ticker']).sort_index(level=[0,1], ascending=[True, False])\n",
    "        sf1.drop(['datekey','reportperiod','lastupdated','dimension'],axis=1,inplace=True)\n",
    "        os.remove(item)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1 = UT.cleaning_dataframe(sf1,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1['roe'] = sf1['netinc'] / sf1['equity']\n",
    "sf1['roa'] = sf1['netinc'] / sf1['assets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 5 - Daily metrics data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue to download data, here get daily metrics SHARADAR/DAILY data and store this table as a zip file on the local drive. This table gives us the data that with discussed in previously started at `start_sep` defined in the parameters section and ended at the current date `tod`. The zip file is extracted, sorted into a multi-index data frame, and finally cleaned using our local function `cleaning_dataframe` imported from `utils_s.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_data:\n",
    "    quandl.export_table('SHARADAR/DAILY',\n",
    "                        ticker=universe, \n",
    "                        date={'gte': start_sep, 'lte': str(tod)}, \n",
    "                        filename='data/daily.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/daily.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "for item in os.listdir(os.getcwd()):  # loop through items in dir\n",
    "    \n",
    "    if item.endswith('.csv') and item.split('_')[0] == 'SHARADAR' and item.split('_')[1] == 'DAILY':\n",
    "        daily = pd.read_csv(item)\n",
    "        daily['date'] = pd.to_datetime(daily['date'])\n",
    "        daily = daily.set_index(['date', 'ticker']).sort_index(level=[0,1], ascending=[True, False])\n",
    "        daily.drop(['lastupdated'],axis=1,inplace=True)\n",
    "        os.remove(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = UT.cleaning_dataframe(daily,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 6 - Sentiment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last data we get is sentiment data IFT/NSA data and store this table as a zip file on the local drive. This table gives us the data that discussed previously started at `start_sep` defined in the parameters section and ended at the current date `tod`. The zip file is extracted, sorted into a multi-index data frame, and finally cleaned using our local function `cleaning_dataframe` imported from `utils_s.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_data:\n",
    "    quandl.export_table('IFT/NSA',\n",
    "                        ticker=universe, \n",
    "                        date={'gte': start_sep, 'lte': str(end)}, \n",
    "                        filename='data/sent_test.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/sent.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "for item in os.listdir(os.getcwd()):  # loop through items in dir\n",
    "    \n",
    "    if item.endswith('.csv') and item.split('_')[0] == 'IFT' and item.split('_')[1] == 'NSA':\n",
    "        sent = pd.read_csv(item)\n",
    "        sent['date'] = pd.to_datetime(sent['date'])\n",
    "        sent = sent.set_index(['date', 'ticker']).sort_index(level=[0,1], ascending=[True, False])\n",
    "        sent = sent[sent['exchange_cd']=='US']\n",
    "        sent.drop(['name','exchange_cd'],axis=1,inplace=True)\n",
    "        os.remove(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = UT.cleaning_dataframe(sent,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Universe intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data is collected and stacked separately into a multi-index data frame. However, some of the assets defined previously in the universe section are dropped or not collected during the download and cleaning process. Therefore, the goal in this section is to get the intersection of assets represented for each data frame (`sent`,`sf1`,`daily` and `ohlcv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_universe =list(set(sent['sentiment'].unstack('ticker').columns) & set(sf1.index.levels[1]) & set(daily.index.levels[1]) & set(ohlcv.index.levels[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} assets dropped after intersectioning'.format((len(universe) - len(new_universe))))\n",
    "print ('{} assets are loaded'.format(len(new_universe)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 1- Sectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sectors represented in the cleaned data are as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cleaned = meta_ex_cu_de_cap.loc[new_universe,:]\n",
    "print ('Sectors in cleaned data: \\n')\n",
    "sectors = {}\n",
    "for i in set(meta_cleaned['sector']):\n",
    "    print (' ',i)\n",
    "    sectors[i] = list(meta_cleaned[meta_cleaned['sector'] == i].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Alpha factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor-driven alpha investment strategies, designed to delivering market-beating returns, come in a number of different forms. In this porject, the following alpha factors are discussed:\n",
    "\n",
    "- Daily Metrics\n",
    "- Simple Moving Average\n",
    "- Overnight Sentiment\n",
    "- Mean Reversion\n",
    "- Direction\n",
    "- Sentiment Analysis\n",
    "- Volatility\n",
    "- Capm\n",
    "- Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 1 - Factorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These factors are generated as followed:\n",
    "\n",
    "1 - Formulation <br>\n",
    "2 - Scaling <br>\n",
    "3 - Smoothing <br>\n",
    "4 - Slicing <br>\n",
    "5 - Neutralizing by Sector <br>\n",
    "6 - Scaling <br>\n",
    "\n",
    "Each of these factors of stored into a dictionary to be fetch into a final multiindex dataframe `all_factors` as the ouput of our pipleline. Once this is done, the multiindex data frame will be used as the input of other functions for analysis and optimization etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = ohlcv['close'].unstack('ticker')[new_universe]\n",
    "openn = ohlcv['open'].unstack('ticker')[new_universe]\n",
    "high = ohlcv['high'].unstack('ticker')[new_universe]\n",
    "low = ohlcv['low'].unstack('ticker')[new_universe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing = openn.loc[slice(start,end),:].tz_localize('UTC')[new_universe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 1 - Daily Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data = {}\n",
    "\n",
    "for i in daily_in.keys():\n",
    "    \n",
    "    df_daily = daily[i].unstack('ticker')\n",
    "    # formulation and scaling\n",
    "    reversion = FM().momentum(df_daily,daily_in[i])*-1\n",
    "    # smoothing\n",
    "    smoothed_reversion = FM().smooth(reversion,smoothed_value)\n",
    "    # slicing\n",
    "    smoothed_reversion.index = pd.to_datetime(smoothed_reversion.index)\n",
    "    smoothed_reversion = smoothed_reversion.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    smoothed_reversion_neutralized_scaled = FM().sector_neutral(sectors, smoothed_reversion)\n",
    "\n",
    "    \n",
    "    daily_data[i] = smoothed_reversion_neutralized_scaled[new_universe]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 2 - Simple Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_data = {}\n",
    "\n",
    "for name,period in sma_in.items():\n",
    "    # formulation and scaling\n",
    "    sma_min = FM().sma(close, period)\n",
    "    # smoothing\n",
    "    smoothed_sma_min = FM().smooth(sma_min,smoothed_value)\n",
    "    # slicing\n",
    "    smoothed_sma_min.index = pd.to_datetime(smoothed_sma_min.index)\n",
    "    smoothed_sma_min = smoothed_sma_min.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    smoothed_sma_min_neutralized_scaled = FM().sector_neutral(sectors, smoothed_sma_min)\n",
    "    sma_data[name] = smoothed_sma_min_neutralized_scaled[new_universe]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 3 - Overnight Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_data = {}\n",
    "\n",
    "for name,period in over_in.items():\n",
    "    # formulation and scaling\n",
    "    overnight_sentiment = FM().overnight_sentiment(close, openn, 2, trailing_window=period)\n",
    "    # smoothing\n",
    "    smoothed_overnight_sentiment = FM().smooth(overnight_sentiment,smoothed_value)\n",
    "    # slicing \n",
    "    smoothed_overnight_sentiment.index = pd.to_datetime(smoothed_overnight_sentiment.index)\n",
    "    smoothed_overnight_sentiment = smoothed_overnight_sentiment.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    smoothed_overnight_sentiment_neutralized_scaled = FM().sector_neutral(sectors, smoothed_overnight_sentiment)\n",
    "    over_data[name] = (smoothed_overnight_sentiment_neutralized_scaled*-1)[new_universe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 4 - Mean Reversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_data = {}\n",
    "\n",
    "for name,period in momentum_in.items():\n",
    "    # formulation and scaling\n",
    "    mean_reversion = FM().momentum(close,period)*-1\n",
    "    # smoothing\n",
    "    smoothed_mean_reversion = FM().smooth(mean_reversion,smoothed_value)\n",
    "    # slicing\n",
    "    smoothed_mean_reversion.index = pd.to_datetime(smoothed_mean_reversion.index)\n",
    "    smoothed_mean_reversion = smoothed_mean_reversion.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    smoothed_mean_reversion_neutralized_scaled = FM().sector_neutral(sectors, smoothed_mean_reversion)\n",
    "    momentum_data[name] = smoothed_mean_reversion_neutralized_scaled[new_universe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 5 - Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_data = {}\n",
    "\n",
    "for name,period in direction_in.items():\n",
    "    \n",
    "    # formulation and scaling\n",
    "    direct = FM().direction(close, openn, 1, period)\n",
    "    # smoothing\n",
    "    smoothed_direct = FM().smooth(direct,smoothed_value)\n",
    "    # slicing\n",
    "    smoothed_direct.index = pd.to_datetime(smoothed_direct.index)\n",
    "    smoothed_direct = smoothed_direct.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    smoothed_direct_neutralized_scaled = FM().sector_neutral(sectors, smoothed_direct)\n",
    "    direction_data[name] = smoothed_direct_neutralized_scaled[new_universe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 6 - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment\n",
    "sent_data = {}\n",
    "\n",
    "for name,period in sent_in.items():\n",
    "    \n",
    "    # formulation and scaling\n",
    "    sentiment = FM().sentiment(close, high, low, sent, period, new_universe)\n",
    "    # smoothing\n",
    "    smoothed_sentiment= FM().smooth(sentiment,smoothed_value)\n",
    "    # slicing\n",
    "    smoothed_sentiment.index = pd.to_datetime(smoothed_sentiment.index)\n",
    "    smoothed_sentiment = smoothed_sentiment.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    smoothed_sentiment_neutralized_scaled = FM().sector_neutral(sectors, smoothed_sentiment)\n",
    "    sent_data[name] = smoothed_sentiment_neutralized_scaled[new_universe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 7 - Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_data = {}\n",
    "\n",
    "for name,period in vol_in.items():\n",
    "    \n",
    "    # formulation and scaling\n",
    "    vol = FM().volatility(close, 5, period)\n",
    "    # smoothing\n",
    "    smoothed_vol = FM().smooth(vol,smoothed_value)\n",
    "    # slicing\n",
    "    smoothed_vol.index = pd.to_datetime(smoothed_vol.index)\n",
    "    smoothed_vol = smoothed_vol.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    smoothed_vol_neutralized_scaled = FM().sector_neutral(sectors, smoothed_vol)\n",
    "    volatility_data[name] = smoothed_vol_neutralized_scaled[new_universe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 8 - Capm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_data = {}\n",
    "\n",
    "for name,period in capm_in.items():\n",
    "    \n",
    "    # formulation and scaling\n",
    "    cap = FM().capm(close, dfm[['close']], 1, period)\n",
    "    # smoothing\n",
    "    smoothed_cap = FM().smooth(cap,smoothed_value)\n",
    "    # slicing\n",
    "    smoothed_cap.index = pd.to_datetime(smoothed_cap.index)\n",
    "    smoothed_cap = smoothed_cap.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    #smoothed_cap_neutralized_scaled = FM().sector_neutral(sectors, smoothed_cap)\n",
    "    capm_data[name] = smoothed_cap[new_universe]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 9 - Channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_data = {}\n",
    "\n",
    "for name,period in channels_in.items():\n",
    "    \n",
    "    # formulation and scaling\n",
    "    chan = FM().channels(close, period)\n",
    "    # smoothing\n",
    "    smoothed_chan = FM().smooth(chan,20)\n",
    "    # slicing\n",
    "    smoothed_chan.index = pd.to_datetime(smoothed_chan.index)\n",
    "    smoothed_chan = smoothed_chan.loc[slice(start,end),:]\n",
    "    # neutralizing and scaling\n",
    "    smoothed_chan_neutralized_scaled = FM().sector_neutral(sectors, smoothed_chan)\n",
    "    chan_data[name] = smoothed_chan_neutralized_scaled[new_universe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - 1 - 10 - Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_data = {}\n",
    "\n",
    "for i in fundamental_in:\n",
    "    \n",
    "    df = sf1[i].unstack('ticker')[new_universe]\n",
    "    df.fillna(df.mean(axis=0),inplace=True)\n",
    "    \n",
    "    # formulation\n",
    "    returns = FM().returns(df,1)\n",
    "    returns.replace([np.inf, -np.inf], np.nan, inplace=True)    \n",
    "\n",
    "    # neutralizing and scaling\n",
    "    returns_neutralize_scaled = FM().sector_neutral(sectors, returns)\n",
    "    \n",
    "    chunk = (ohlcv.index.levels[0][-1]+datetime.timedelta(days=1)).date()\n",
    "    chunk_minus = ohlcv.index.levels[0][-1].date()\n",
    "    # resampling\n",
    "    if datetime.datetime.strptime(sf1.index.levels[0][-1], '%Y-%m-%d').timestamp() > ohlcv.index.levels[0][-1].timestamp():\n",
    "        pass\n",
    "    else:\n",
    "        returns_neutralize_scaled.loc[chunk,:] = np.nan\n",
    "        \n",
    "    returns_neutralize_scaled.index = pd.to_datetime(returns_neutralize_scaled.index)\n",
    "    returns_neutralize_resampled = returns_neutralize_scaled.resample('D').pad()\n",
    "    returns_neutralize_resampled = returns_neutralize_resampled.loc[ohlcv.index.levels[0][0]:chunk_minus,:]\n",
    "    \n",
    "    # drop holidays\n",
    "    holidays = returns_neutralize_resampled.index ^ ohlcv.index.levels[0]\n",
    "    returns_neutralize_resampled.drop(list(holidays),axis=0,inplace = True)\n",
    "    returns_neutralize_resampled.index.name = ohlcv.index.levels[0].name\n",
    "    \n",
    "    # slicing\n",
    "    returns_neutralize_resampled_1y = returns_neutralize_resampled.loc[slice(start,end),:]\n",
    "\n",
    "    fund_data[i] = returns_neutralize_resampled_1y[new_universe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 4 - Multiindex factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, factor data are unpacked and stacked into a multi-index data frame to finally be regrouped in a final multi-index data frame `all_factors`. The index is composed of two levels respectively `date` and `ticker`. The column is composed of factors created in previous sections relative to the date and ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for i in fundamental_in:\n",
    "    \n",
    "    fund_return_neutralized_stacked = fund_data[i].stack().to_frame(i)\n",
    "    result.append(fund_return_neutralized_stacked)\n",
    "\n",
    "for i in daily_in.keys():\n",
    "\n",
    "    daily_stacked = daily_data[i].stack().to_frame('daily_{}{}days'.format(i,daily_in[i]))\n",
    "    result.append(daily_stacked)\n",
    "\n",
    "for i in sma_data.keys():\n",
    "\n",
    "    sma_stacked = sma_data[i].stack().to_frame('sma{}'.format(sma_in[i]))\n",
    "    result.append(sma_stacked)\n",
    "\n",
    "for i in momentum_data.keys():\n",
    "    \n",
    "    momentum_stacked = momentum_data[i].stack().to_frame('momentum{}days'.format(momentum_in[i]))\n",
    "    result.append(momentum_stacked)\n",
    "    \n",
    "for i in over_data.keys():\n",
    "    \n",
    "    over_stacked = over_data[i].stack().to_frame('overnight_sent{}days'.format(over_in[i]))\n",
    "    result.append(over_stacked)    \n",
    "\n",
    "for i in direction_data.keys():\n",
    "    \n",
    "    direct_stacked = direction_data[i].stack().to_frame('direction{}days'.format(direction_in[i]))\n",
    "    result.append(direct_stacked)\n",
    "    \n",
    "for i in volatility_data.keys():\n",
    "    \n",
    "    vol_stacked = volatility_data[i].stack().to_frame('volatility{}days'.format(vol_in[i]))\n",
    "    result.append(vol_stacked)\n",
    "    \n",
    "for i in capm_data.keys():\n",
    "    \n",
    "    capm_stacked = capm_data[i].stack().to_frame('capm{}days'.format(capm_in[i]))\n",
    "    result.append(capm_stacked)\n",
    "    \n",
    "for i in chan_data.keys():\n",
    "    \n",
    "    chan_stacked = chan_data[i].stack().to_frame('channels{}days'.format(channels_in[i]))\n",
    "    result.append(chan_stacked)\n",
    "    \n",
    "for i in sent_data.keys():\n",
    "    \n",
    "    sent_stacked = sent_data[i].stack().to_frame('sentiment{}days'.format(sent_in[i]))\n",
    "    result.append(sent_stacked)    \n",
    "    \n",
    "all_factors = pd.concat(result,axis=1)\n",
    "\n",
    "all_factors.index.set_names(['date', 'asset'], inplace=True)\n",
    "\n",
    "all_factors.index = all_factors.index\\\n",
    "                               .set_levels([all_factors.index.levels[0].tz_localize('UTC'), all_factors.index.levels[1]])\n",
    "\n",
    "all_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors['ncf_reversed'] = all_factors['ncf'] * -1\n",
    "#all_factors['fcfps_reversed'] = all_factors['fcfps'] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan values per columns\n",
    "all_factors.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - All factors analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have processed and regrouped factor data, we are ready to analyze the factor one by one to see if they have the potential to be combined or not. In this context, [alphalens](https://quantopian.github.io/alphalens/index.html) is used for the analysis. This package regrouped APIs useful for data processing and factor analysis over the pre-defined period `rebalance_period`. These metrics are as followed:\n",
    "\n",
    "\n",
    "- <b>Cleaning and preparing data</b> `alphalens.utils.get_clean_factor_and_forward_returns`: Formats the factor data, pricing data, and group mappings into a DataFrame that contains aligned MultiIndex indices of timestamp and asset. The returned data will be formatted to be suitable for Alphalens functions. \n",
    "\n",
    "- <b>Cumulated factor return</b> `alphalens.performance.factor_returns`: Builds cumulative returns from ‘period’ returns. This function simulate the cumulative effect that a series of gains or losses (the ‘returns’) have on an original amount of capital over a period of time.\n",
    "\n",
    "-  <b>Mean quantile return</b> `alphalens.performance.mean_return_by_quantile`: Computes mean returns for factor quantiles across provided forward returns columns.\n",
    "\n",
    "- <b>Factor Rank Autocorrelation</b> `alphalens.performance.factor_rank_autocorrelation`: Computes autocorrelation of mean factor ranks in specified time spans. We must compare period to period factor ranks rather than factor values to account for systematic shifts in the factor values of all names or names within a group. This metric is useful for measuring the turnover of a factor. If the value of a factor for each name changes randomly from period to period, we’d expect an autocorrelation of 0.\n",
    "\n",
    "- <b>Sharpe ratio</b> `sharpe_ratio`: This function computes annualized sharpe ratio. This metric is used to understand the return of an investment compared to its risk. The ratio is the average return earned in excess per unit of volatility or total risk. Volatility is a measure of the factor return fluctuations of an asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Rebalance period set to {} days for all factors analysis'.format(rebalance_period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_factor_data = {\n",
    "    factor: al.utils.get_clean_factor_and_forward_returns(factor=factor_data, \n",
    "                                                          prices=pricing, \n",
    "                                                          periods=[rebalance_period], \n",
    "                                                          quantiles=5,\n",
    "                                                          filter_zscore=20,\n",
    "                                                          max_loss=0.35)\n",
    "    for factor, factor_data in all_factors[['capm20days',\n",
    "                                            'sma200',\n",
    "                                            'daily_ps100days',\n",
    "                                            #'daily_pb100days',\n",
    "                                            'direction100days',\n",
    "                                            'momentum252days',\n",
    "                                            'sentiment10days',\n",
    "                                            'overnight_sent60days',\n",
    "                                            'volatility20days',\n",
    "                                            'daily_marketcap120days',\n",
    "                                            'daily_evebitda100days',\n",
    "                                            #'daily_pe100days',\n",
    "                                            'channels100days',\n",
    "                                         ]].iteritems()}\n",
    "\n",
    "unixt_factor_data = {\n",
    "    factor: factor_data.set_index(pd.MultiIndex.from_tuples(\n",
    "        [(x.timestamp(), y) for x, y in factor_data.index.values],\n",
    "        names=['date', 'asset']))\n",
    "    for factor, factor_data in clean_factor_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - 1 - Cumulated factor return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (25, 15)\n",
    "\n",
    "ls_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in clean_factor_data.items():\n",
    "    ls_factor_returns[factor] = al.performance.factor_returns(factor_data).iloc[:, 0]\n",
    "\n",
    "plt.plot((1+ls_factor_returns).cumprod(), lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - 2 - Quantile analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in unixt_factor_data.items():\n",
    "    qr_factor_returns[factor] = al.performance.mean_return_by_quantile(factor_data)[0].iloc[:, 0]\n",
    "\n",
    "(10000*qr_factor_returns).plot.bar(\n",
    "    subplots=True,\n",
    "    sharey=True,\n",
    "    layout=(20,2),\n",
    "    figsize=(14, 50),\n",
    "    legend=False, fontsize=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - 3 - Factor Rank Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_FRA = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in unixt_factor_data.items():\n",
    "    ls_FRA[factor] = al.performance.factor_rank_autocorrelation(factor_data,period=rebalance_period)\n",
    "\n",
    "plt.plot(ls_FRA,lw=2)\n",
    "plt.title(\"Factor Rank Autocorrelation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - 4 - Sharpe ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(factor_returns, annualization_factor):\n",
    "\n",
    "    df_sharpe = pd.Series(annualization_factor*factor_returns.mean()/factor_returns.std())\n",
    "    \n",
    "    return df_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_annualization_factor = np.sqrt(252)\n",
    "df_sharpe = sharpe_ratio(ls_factor_returns, daily_annualization_factor).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sharpe.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Combined factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - 1 - Combining selceted factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "selected_factors = [\n",
    "                         'sma200',\n",
    "                         #'daily_ps100days',\n",
    "                         #'daily_pb100days',\n",
    "                         'direction100days',\n",
    "                         'momentum252days',\n",
    "                         #'sentiment10days',\n",
    "                         #'overnight_sent60days',\n",
    "                         #'volatility20days',\n",
    "                         'daily_marketcap120days',\n",
    "                         #'daily_evebitda100days',\n",
    "                         #'daily_pe100days',\n",
    "                         #'capm20days'\n",
    "                         'channels100days'\n",
    "                   ]\n",
    "\n",
    "print('Selected Factors:\\n{} '.format(',\\n'.join(selected_factors)))\n",
    "\n",
    "all_factors = all_factors[selected_factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors.to_csv('data/all_factors_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = LE().feature_importance_xgb(n_fwd_days = rebalance_period, \n",
    "                                                         close = close, \n",
    "                                                         all_factors = all_factors,\n",
    "                                                         lower_percentile = 40,\n",
    "                                                         upper_percentile = 60,\n",
    "                                                         n_estimators = 150, \n",
    "                                                         train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances.sort_values(by='weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors_copy = all_factors.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for factor in selected_factors:\n",
    "    all_factors_copy.loc[:,factor] = feature_importances.loc[factor][0] * all_factors.loc[:,factor]\n",
    "all_factors_copy.loc[:,'alpha_vector'] = all_factors.sum(axis=1)\n",
    "all_factors = all_factors_copy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = pd.DataFrame(data=all_factors['alpha_vector'],columns = ['alpha_vector','sector'])\n",
    "for date in vec.index.levels[0]:\n",
    "    vec.loc[date,['sector']] = meta_ex_cu_de_cap.loc[vec.index.levels[1]]['sector'].values\n",
    "sectors = vec['sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6 - 2 - Creating clean factor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = al.utils.get_clean_factor_and_forward_returns(factor = all_factors['alpha_vector'], \n",
    "                                                          prices = pricing, \n",
    "                                                          periods = combined_periods,\n",
    "                                                          quantiles = qunatile_portions,\n",
    "                                                          groupby=sectors,\n",
    "                                                          binning_by_group=False,\n",
    "                                                          filter_zscore=20,\n",
    "                                                          max_loss=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - 3 - Creating tear sheets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_returns, pf_positions, pf_benchmark = \\\n",
    "al.performance.create_pyfolio_input(factor_data,\n",
    "                                       period='{}D'.format(rebalance_period),\n",
    "                                       capital=1,\n",
    "                                       long_short=True,\n",
    "                                       group_neutral=False,\n",
    "                                       equal_weight=False,\n",
    "                                       #quantiles=[1,2,4,5],\n",
    "                                       groups=sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_benchmark = benchmark.loc[slice(pf_returns.index[0],pf_returns.index[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_mappings = sectors.loc[pd.IndexSlice[pf_returns.index[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_mappings = {}\n",
    "for i in sec_mappings.index:\n",
    "    sector_mappings[i] = sec_mappings.loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pf.create_full_tear_sheet(returns = pf_returns, \n",
    "#                           positions = pf_positions,\n",
    "#                           sector_mappings = sector_mappings,\n",
    "#                           benchmark_rets = pf_benchmark )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al.tears.create_full_tear_sheet(factor_data, by_group=True, long_short=True, group_neutral=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_factor_returns = pd.DataFrame()\n",
    "\n",
    "qr_factor_returns = al.performance.mean_return_by_quantile(factor_data)[0]\n",
    "\n",
    "(10000*qr_factor_returns).plot.bar(\n",
    "    subplots=True,\n",
    "    sharey=True,\n",
    "    layout=(4,2),\n",
    "    figsize=(14, 14),\n",
    "    legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_factor_returns = al.performance.factor_returns(factor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_annualization_factor = np.sqrt(252)\n",
    "sharpe_ratio(ls_factor_returns, daily_annualization_factor).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 -  Risk analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame()\n",
    "dff['factor'] = all_factors['alpha_vector']\n",
    "df_all_weights = al.performance.factor_weights(dff, demeaned=True, group_adjust=False, equal_weight=False)\n",
    "all_weights = df_all_weights.loc[pd.IndexSlice[all_factors.index.levels[0][-1]]]\n",
    "all_weights = pd.DataFrame(data = all_weights.values, \n",
    "                                           columns = ['optimal_weights'],\n",
    "                                           index = all_weights.index)\n",
    "all_weights.index.name = 'asset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = all_weights.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_portfolio_risk,Risk_Model = RM().portfolio_risk(close[assets],num_factor_exposures=13,weights=all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Predicted Risk: {} %'.format(np.round((predicted_portfolio_risk*100),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Integrating factor data to optimzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once alpha model and a risk model are generated, we want to find a portfolio that trades as close as possible to the alpha model but limiting risk as measured by the [risk_model](https://github.com/keyvantaj/Quantitative/blob/master/risk_model.py). The [cxpy](https://www.cvxpy.org/) package is used to implement the [optimizer](https://github.com/keyvantaj/Quantitative/blob/master/optimizer.py)\n",
    "\n",
    "The CVXPY objective function is to maximize 𝛼𝑇 ∗ 𝑥 , where x is the portfolio weights and alpha is the alpha vector.\n",
    "\n",
    "In the other hand we have the following constraints:\n",
    "\n",
    "- $ r \\leq risk_{\\text{cap}}^2 \\\\ $\n",
    "- $ B^T * x \\preceq factor_{\\text{max}} \\\\ $\n",
    "- $ B^T * x \\succeq factor_{\\text{min}} \\\\ $\n",
    "- $ x^T\\mathbb{1} = 0 \\\\ $\n",
    "- $ \\|x\\|_1 \\leq 1 \\\\ $\n",
    "- $ x \\succeq weights_{\\text{min}} \\\\ $\n",
    "- $ x \\preceq weights_{\\text{max}} $\n",
    "\n",
    "Where x is the portfolio weights, B is the factor betas, and r is the portfolio risk calculated in [risk model](https://github.com/keyvantaj/Quantitative/blob/master/risk_model.py) module.\n",
    "\n",
    "The first constraint is that the predicted risk be less than some maximum limit. The second and third constraints are on the maximum and minimum portfolio factor exposures. The fourth constraint is the \"market neutral constraint: the sum of the weights must be zero. The fifth constraint is the leverage constraint: the sum of the absolute value of the weights must be less than or equal to 1.0. The last are some minimum and maximum limits on individual holdings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal = pd.DataFrame(index = all_factors.index.levels[0], columns = all_factors.index.levels[1])\n",
    "for date in all_factors.index.levels[0]:\n",
    "    \n",
    "    x = all_factors[['alpha_vector']].loc[date,:]\n",
    "    optimal.loc[date] = OHR(lambda_reg = lambda_reg,\n",
    "                                         factor_max = factor_max, factor_min = factor_min, \n",
    "                                         weights_max = weights_max, weights_min = weights_min,\n",
    "                                         risk_cap = risk_cap).find(\n",
    "                                                                    x, \n",
    "                                                                    Risk_Model['factor_betas'], \n",
    "                                                                    Risk_Model['factor_cov_matrix'], \n",
    "                                                                    Risk_Model['idiosyncratic_var_vector']).values.flatten()\n",
    "    \n",
    "optimal = optimal.astype(np.float)\n",
    "optimal_stacked = optimal.stack().to_frame('optimal_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert optimal_stacked.shape[0] == vec.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - 1 - Quantile data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the factor analysis of the optimized alpha vector, we will define quantiles for data of each date. For this purpose, we use the `qunatile_portions` parameter defined previously to cut data with specific portions. Then we iterate over `optimal_stacked` and apply [pandas qcut](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.qcut.html) function for each date to finally stacked the data into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_optimal_stacked = UT.quantilize(qunatile_portions, \n",
    "                                              optimal_stacked,\n",
    "                                              weights_col='optimal_weights',\n",
    "                                              q_col='quantile',\n",
    "                                              sec_col='sector',\n",
    "                                              sec_df=vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('quantiles:', list(set(quantile_optimal_stacked['quantile'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - 2 - Quantiles indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to select the edges using `quantile_to_analyse` previously define in <b>parameters</b> section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_final_vector = UT.q_indexing(quantile_to_analyse, quantile_optimal_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_q_dropped = ((quantile_optimal_stacked.shape[0] - q_final_vector.shape[0])/quantile_optimal_stacked.shape[0])*100\n",
    "print ('{} % dropped after quantile select'.format(np.round(percent_q_dropped,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('selected quantiles:', list(set(q_final_vector['quantile'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - 3 - Sectors Selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vector,sectors = UT.select_sector(q_final_vector, drop_long_sec, drop_short_sec,\n",
    "                                        sec_col='sector',factor_col = 'quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} rows dropped after sector select'.format(q_final_vector.shape[0] - final_vector.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Optimized alpha vector analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - 1 - Creating clean factor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = al.utils.get_clean_factor_and_forward_returns(factor = final_vector['optimal_weights'], \n",
    "                                                          prices = pricing, \n",
    "                                                          periods = combined_periods,\n",
    "                                                          quantiles = len(quantile_to_analyse),\n",
    "                                                          groupby=sectors,\n",
    "                                                          binning_by_group=False,\n",
    "                                                          filter_zscore=20,\n",
    "                                                          max_loss=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - 2 - Sector selection for factor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data, sectors = UT.select_sector(factor_data, drop_long_sec, drop_short_sec,\n",
    "                                     sec_col='group',factor_col = 'factor_quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} rows dropped from factor data'.format(factor_data.shape[0] - factor_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - 3 - Creating tear sheets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_returns, pf_positions, pf_benchmark = \\\n",
    "al.performance.create_pyfolio_input(factor_data,\n",
    "                                   period = '{}D'.format(rebalance_period),\n",
    "                                   capital = 1,\n",
    "                                   long_short = True,\n",
    "                                   group_neutral = False,\n",
    "                                   equal_weight = False,\n",
    "                                   groups = sectors)\n",
    "\n",
    "pf_benchmark = benchmark.loc[slice(pf_returns.index[0],pf_returns.index[-1])]\n",
    "sec_mappings = sectors.loc[pd.IndexSlice[pf_returns.index[0]]]\n",
    "\n",
    "sector_mappings = {}\n",
    "for i in sec_mappings.index:\n",
    "    sector_mappings[i] = sec_mappings.loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pf.create_full_tear_sheet(returns = pf_returns, \n",
    "#                           positions = pf_positions,\n",
    "#                           sector_mappings = sector_mappings,\n",
    "#                           benchmark_rets = pf_benchmark,\n",
    "#                           factor_returns = factor_data[['factor']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "al.tears.create_full_tear_sheet(factor_data, by_group=True, long_short=True, group_neutral=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_factor_returns = al.performance.factor_returns(factor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_annualization_factor = np.sqrt(252)\n",
    "sharpe_ratio(ls_factor_returns, daily_annualization_factor).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Predicted Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_regularized = final_vector[['optimal_weights']].loc[final_vector.index.levels[0][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('for {} assets  with end date: {}'.format(len(optimal_weights_regularized.index),optimal.index[-1]))\n",
    "optimal_weights_regularized.plot.bar(legend=None, title='Portfolio % Holdings by Stock')\n",
    "plt.grid(alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_regularized = pd.DataFrame(data = optimal_weights_regularized.values, \n",
    "                                           columns = ['optimal_weights'],\n",
    "                                           index = optimal_weights_regularized.index)\n",
    "optimal_weights_regularized.index.name = 'asset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = optimal_weights_regularized.sort_values(by='optimal_weights',ascending=False)[:len(optimal_weights_regularized.index)//5]\n",
    "q1 = optimal_weights_regularized.sort_values(by='optimal_weights',ascending=True)[:len(optimal_weights_regularized.index)//5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (q5.iloc[0])\n",
    "print (q1.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - 1 - Risk analysis with optimized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_portfolio_risk,Risk_Model = RM().portfolio_risk(close[optimal_weights_regularized.index],num_factor_exposures=factor_exposures,weights=optimal_weights_regularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Predicted Risk: {} %'.format(np.round((predicted_portfolio_risk*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close.to_csv('data/close.csv')\n",
    "optimal_weights_regularized.to_csv('output/optimal_weights_regularized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('the total leverage is {}'.format(optimal_weights_regularized.abs().sum().round(2)[0]))\n",
    "print ('the long/short leverage balance is {}'.format(optimal_weights_regularized.sum().round(2)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
